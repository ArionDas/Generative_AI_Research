# Generative_AI_Research
This repository contains SOTA concepts derived from top tier research papers in the field of Generative AI
<br>
### LLMs can't plan, but can help planning in LLM-Modulo frameworks
https://arxiv.org/abs/2402.01817
<br>
### Graph based prompt optimization using DsPY
https://www.linkedin.com/posts/maryammiradi_possible-replacement-of-prompt-hacking-graph-based-activity-7206613132205535234-jCfw?utm_source=share&utm_medium=member_desktop
<br>
### Open-Endedness is Essential for Artificial Superhuman Intelligence
https://www.linkedin.com/posts/raphaelmansuy_open-endedness-is-essential-for-ai-activity-7205180687614705664-P8ux?utm_source=share&utm_medium=member_desktop
<br>
### Intention based Embedding Model for text messages
https://www.linkedin.com/posts/pierre-louis-biojout-9509741aa_intent-embed-capturing-user-intention-in-activity-7207014246386151425-o4VX?utm_source=share&utm_medium=member_desktop
<br>
### LLM applications from Regulatory Documents
https://www.linkedin.com/pulse/building-llm-applications-regulatory-documents-insights-cameron-duffy-umzzc/
<br>
### Computer Vision + LLM Models
https://www.linkedin.com/pulse/convergence-computer-vision-llm-models-unlocking-new-text-ko%C3%AFvogui-ujfie/
<br>
### Zero Redundancy Optimizer
https://www.linkedin.com/posts/pramodith_zero-redundancy-optimizer-zero-and-fully-activity-7206963884274925569-S5Ei?utm_source=share&utm_medium=member_desktop
<br>
### TEXTGRAD : Optimizing AI systems with NL Feedback
https://www.linkedin.com/posts/raphaelmansuy_introducing-textgrad-optimizing-ai-systems-activity-7207683259718119428-yQtX?utm_source=share&utm_medium=member_desktop
<br>
### LLM + MCTS for mathematics
https://www.linkedin.com/posts/ownyourai_ai-experts-llms-cant-do-mathematics-ai-activity-7208048810923589632-DAcT?utm_source=share&utm_medium=member_desktop
<br>
### Continual Pre-training of LLMs
https://www.linkedin.com/posts/shamane-siriwardhana_continual-pre-training-of-large-language-activity-7208048325621633024-o9UP?utm_source=share&utm_medium=member_desktop
<br>
### Discovering Preference Optimization Algorithms with & for LLMs
https://arxiv.org/abs/2406.08414
<br>
### SelfGoal : Achieving High Level Goals using Language Agents
https://arxiv.org/abs/2406.04784
<br>
### Mixture of Agents enhancing LLM capabilities
https://arxiv.org/abs/2406.04692
<br>
### Self Tuning with LLMs
https://arxiv.org/pdf/2406.06326
<br>
### MultiModal Table Understanding
https://arxiv.org/abs/2406.08100
<br>
### LLMs providing feedback on research articles
https://www.linkedin.com/posts/rajeswaran-v_we-know-that-ai-is-helping-people-write-papers-activity-7208272147134205953-NfzS?utm_source=share&utm_medium=member_desktop
<br>
### ChatGPT is bull#h#t
https://www.linkedin.com/posts/activity-7208121146024628224-on7o?utm_source=share&utm_medium=member_desktop
<br>
### Is GPT4o good at Table recognition
https://www.linkedin.com/posts/shengyun-anthony-peng_machinelearning-gpt-table-activity-7196198710643441664-edNB?utm_source=share&utm_medium=member_desktop
<br>

# LLM ARCHITECTURE
### MultiModal AI Architectures Evolution**
https://www.linkedin.com/feed/update/urn:li:activity:7202977920020004867?utm_source=share&utm_medium=member_desktop
<br>
### Mixture of Agents
https://www.linkedin.com/posts/togethercomputer_mixture-of-agentsa-framework-that-leverages-activity-7206337712671379456-z2ci?utm_source=share&utm_medium=member_desktop
<br>
### Cohere int8 & binary embeddings
https://cohere.com/blog/int8-binary-embeddings
<br>
### Llama3 from scratch
https://www.linkedin.com/posts/naklecha_i-implemented-llama3-from-scratch-every-activity-7198270292970872832-2frV?utm_source=share&utm_medium=member_desktop
<br>
### LAMINI Memory Tuning
https://www.linkedin.com/posts/omarsar_huge-if-true-basically-the-way-they-significantly-activity-7207410659947352066-dNDR?utm_source=share&utm_medium=member_desktop
<br>
### Consistent Middle Enhancement in LLMs
https://arxiv.org/abs/2406.07138
<br>
### Mat-Mul Free Language Modeling
https://www.linkedin.com/posts/omarsar_matmul-free-llms-proposes-an-implementation-activity-7204139829998010368-8XMA?utm_source=share&utm_medium=member_desktop
<br>

# OPEN SOURCE LLMs
### NEMOTRON : A 340B transformer from NVIDIA
https://www.linkedin.com/posts/pascalbiese_nemotron-4-340b-technical-report-activity-7208919714608144385--n0V?utm_source=share&utm_medium=member_desktop
<br>
# LLM REASONING
### Q* Multi-step Reasoning in AI
https://www.linkedin.com/posts/raphaelmansuy_q-improving-multi-step-reasoning-for-llms-activity-7210463942932873217-B6D3?utm_source=share&utm_medium=member_desktop
<br>
### LLMs for Probabilistic Reasoning
https://www.linkedin.com/posts/raphaelmansuy_llms-are-capable-of-probabilistic-reasoning-activity-7209518362681327617-cnXC?utm_source=share&utm_medium=member_desktop
<br>
### Predictable Memorization in LLMs
https://www.linkedin.com/posts/bommarito_llms-dont-memorize-things-they-grok-activity-7207698156963131392-XXmF?utm_source=share&utm_medium=member_desktop
<br>
### How far can Transformers reason
https://www.linkedin.com/posts/raphaelmansuy_ai-how-far-can-transformers-reason-activity-7206452021636255745-5_7W?utm_source=share&utm_medium=member_desktop
<br>
### Planning Abilities of LLMs
https://proceedings.neurips.cc/paper_files/paper/2023/hash/efb2072a358cefb75886a315a6fcf880-Abstract-Conference.html
<br>
### Tree Search for Language Model Agents
https://jykoh.com/search-agents/paper.pdf
<br>

# RAG
### Graph RAG
https://www.linkedin.com/posts/sarthakrastogi_graph-rag-can-perform-much-better-than-std-activity-7206988711329222657-Okf0?utm_source=share&utm_medium=member_desktop
<br>
### Corrective RAG
https://www.linkedin.com/posts/tonyseale_corrective-retrieval-augmented-generation-activity-7207290670552600577-r3QG?utm_source=share&utm_medium=member_desktop
<br>
### RAG + Knowledge Graph
https://www.linkedin.com/posts/svonava_rag-knowledge-graphs-cut-customer-support-activity-7207397265425006593-t2-c?utm_source=share&utm_medium=member_desktop
<br>
### PlanRAG
https://arxiv.org/abs/2406.12430
<br>
### From RAG to rich parameters
https://arxiv.org/abs/2406.12824
<br>

# LONG CONTEXT PROCESSING
### Infini Transformer
https://arxiv.org/abs/2404.07143
<br>
### GraphReader
https://www.linkedin.com/posts/raphaelmansuy_graphreaderlong-context-processing-in-ai-activity-7209866207464718338-ww_p?utm_source=share&utm_medium=member_desktop
<br>
### Can Long-Context Language Models Subsume Retrieval, RAG, SQL ?
https://www.linkedin.com/posts/omarsar_can-long-context-language-models-subsume-activity-7209957073315315712-c1Bq?utm_source=share&utm_medium=member_desktop

# HALLUCINATION MITIGATION
### Semantic Entropy
chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.nature.com/articles/s41586-024-07421-0.pdf
<br>
### Banishing Hallucinations : LAMINI
https://www.linkedin.com/posts/pierre-louis-biojout-9509741aa_intent-embed-capturing-user-intention-in-activity-7207014246386151425-o4VX?utm_source=share&utm_medium=member_desktop
<br>

# LLM OPTIMIZATION
### LLMs are serving 20000 queries per second
https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_how-characterais-llms-serve-20000-queries-activity-7209570013735702528-xsgT?utm_source=share&utm_medium=member_desktop
<br>
### Mitigating Memorization in LLMs
https://arxiv.org/abs/2406.10209
<br>
